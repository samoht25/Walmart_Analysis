{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Walmart_Project\n"
     ]
    }
   ],
   "source": [
    "cd \"Walmart_Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#formulas to test out empty and null values\n",
    "def empty_count(data, feature):\n",
    "    empty_mask = data[feature].isnull()\n",
    "    empty_count = len(data[feature][empty_mask])\n",
    "    return empty_count\n",
    "\n",
    "def empty_count_total(data):\n",
    "    for feature in data.columns:\n",
    "        empty_count1 = empty_count(data,feature)\n",
    "        if empty_count1 > 0:\n",
    "            print(feature, empty_count1)\n",
    "            \n",
    "def empty_feature(data):\n",
    "    list_feature=[]\n",
    "    for feature in data.columns:\n",
    "        empty_count1 = empty_count(data,feature)\n",
    "        if empty_count1 > 0:\n",
    "            list_feature.append(feature)\n",
    "    return list_feature\n",
    "\n",
    "#import csv\n",
    "train_df = pd.read_csv(\"data/train_full.csv\")\n",
    "test_df = pd.read_csv(\"data/test_full.csv\")\n",
    "\n",
    "train_df.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n",
    "test_df.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n",
    "\n",
    "train_df.IsHoliday = train_df.IsHoliday.astype(int)\n",
    "\n",
    "#change to category: \n",
    "train_df.Store = train_df.Store.astype(\"category\")\n",
    "test_df.Store  = test_df.Store.astype(\"category\")\n",
    "\n",
    "train_df.Date = train_df.Date.astype(\"category\")\n",
    "test_df.Date  = test_df.Date.astype(\"category\")\n",
    "\n",
    "train_df.Dept = train_df.Dept.astype(\"category\")\n",
    "test_df.Dept  = test_df.Dept.astype(\"category\")\n",
    "\n",
    "train_df.IsHoliday = train_df.IsHoliday.astype(\"category\")\n",
    "test_df.IsHoliday  = test_df.IsHoliday.astype(\"category\")\n",
    "\n",
    "train_df.Type = train_df.Type.astype(\"category\")\n",
    "test_df.Type  = test_df.Type.astype(\"category\")\n",
    "\n",
    "#include Date as a numeric dataset later. \n",
    "train_df.Date = pd.to_datetime(train_df.Date)\n",
    "test_df.Date = pd.to_datetime(test_df.Date)\n",
    "\n",
    "#add year, date, and month columns\n",
    "train_df[\"Year\"] = pd.to_datetime(train_df[\"Date\"], format=\"%Y-%m-%d\").dt.year\n",
    "test_df[\"Year\"] = pd.to_datetime(test_df[\"Date\"], format=\"%Y-%m-%d\").dt.year\n",
    "\n",
    "train_df[\"Month\"] = pd.to_datetime(train_df[\"Date\"], format=\"%Y-%m-%d\").dt.month\n",
    "test_df[\"Month\"] = pd.to_datetime(test_df[\"Date\"], format=\"%Y-%m-%d\").dt.month\n",
    "\n",
    "train_df[\"Day\"] = pd.to_datetime(train_df[\"Date\"], format=\"%Y-%m-%d\").dt.day\n",
    "test_df[\"Day\"] = pd.to_datetime(test_df[\"Date\"], format=\"%Y-%m-%d\").dt.day\n",
    "\n",
    "#add Year, Month, and Day as category\n",
    "train_df[\"Year\"] = train_df[\"Year\"].astype(\"category\")\n",
    "test_df[\"Year\"] = test_df[\"Year\"].astype(\"category\")\n",
    "\n",
    "train_df[\"Month\"] = train_df[\"Month\"].astype(\"category\")\n",
    "test_df[\"Month\"] = test_df[\"Month\"].astype(\"category\")\n",
    "\n",
    "train_df[\"Day\"] = train_df[\"Day\"].astype(\"category\")\n",
    "test_df[\"Day\"] = test_df[\"Day\"].astype(\"category\")\n",
    "\n",
    "#Replace empty nulls with training set's mean. \n",
    "train_df[\"MarkDown1\"] = train_df[\"MarkDown1\"].fillna(train_df[\"MarkDown1\"].mean())\n",
    "train_df[\"MarkDown2\"] = train_df[\"MarkDown2\"].fillna(train_df[\"MarkDown2\"].mean())\n",
    "train_df[\"MarkDown3\"] = train_df[\"MarkDown3\"].fillna(train_df[\"MarkDown3\"].mean())\n",
    "train_df[\"MarkDown4\"] = train_df[\"MarkDown4\"].fillna(train_df[\"MarkDown4\"].mean())\n",
    "train_df[\"MarkDown5\"] = train_df[\"MarkDown5\"].fillna(train_df[\"MarkDown5\"].mean())\n",
    "\n",
    "#test_df make sure to filter the null values with the average from the training set. \n",
    "test_df[\"MarkDown1\"] = test_df[\"MarkDown1\"].fillna(train_df[\"MarkDown1\"].mean())\n",
    "test_df[\"MarkDown2\"] = test_df[\"MarkDown2\"].fillna(train_df[\"MarkDown2\"].mean())\n",
    "test_df[\"MarkDown3\"] = test_df[\"MarkDown3\"].fillna(train_df[\"MarkDown3\"].mean())\n",
    "test_df[\"MarkDown4\"] = test_df[\"MarkDown4\"].fillna(train_df[\"MarkDown4\"].mean())\n",
    "test_df[\"CPI\"] = test_df[\"CPI\"].fillna(train_df[\"CPI\"].mean())\n",
    "test_df[\"Unemployment\"] = test_df[\"Unemployment\"].fillna(train_df[\"Unemployment\"].mean())\n",
    "\n",
    "#numeric data\n",
    "numeric_train_df = train_df.select_dtypes(exclude = \"category\")\n",
    "numeric_test_df = test_df.select_dtypes(exclude = \"category\")\n",
    "\n",
    "#target data\n",
    "target = numeric_train_df[\"Weekly_Sales\"]\n",
    "numeric_train_df.drop(\"Weekly_Sales\", axis=1, inplace=True)\n",
    "\n",
    "#drop Date for numeric train and test as it is NOT a numeric.\n",
    "numeric_train_df.drop(\"Date\", axis=1, inplace=True)\n",
    "numeric_test_df.drop(\"Date\", axis=1, inplace=True)\n",
    "\n",
    "#categorical data\n",
    "categorical_train_df = train_df.select_dtypes(include=\"category\")\n",
    "categorical_train_encoded_df = pd.get_dummies(categorical_train_df)\n",
    "categorical_test_df = train_df.select_dtypes(include=\"category\")\n",
    "categorical_test_encoded_df = pd.get_dummies(categorical_test_df)\n",
    "\n",
    "#categorical significant\n",
    "categorical_train_encoded_stats = pd.DataFrame()\n",
    "categorical_train_encoded_stats[\"mean\"] = categorical_train_encoded_df.mean()\n",
    "categorical_train_encoded_stats[\"std\"] = categorical_train_encoded_df.std()\n",
    "categorical_train_encoded_stats[\"var\"] = categorical_train_encoded_df.var()\n",
    "categorical_train_encoded_stats.sort_values(\"var\", ascending=False).head()\n",
    "\n",
    "categorical_test_encoded_stats = pd.DataFrame()\n",
    "categorical_test_encoded_stats[\"mean\"] = categorical_test_encoded_df.mean()\n",
    "categorical_test_encoded_stats[\"std\"] = categorical_test_encoded_df.std()\n",
    "categorical_test_encoded_stats[\"var\"] = categorical_test_encoded_df.var()\n",
    "categorical_test_encoded_stats.sort_values(\"var\", ascending=False).head()\n",
    "\n",
    "categorical_train_significant = categorical_train_encoded_stats[categorical_train_encoded_stats[\"var\"] > .20].index\n",
    "categorical_train_encoded_sig_df = categorical_train_encoded_df[categorical_train_significant]\n",
    "\n",
    "categorical_test_significant = categorical_test_encoded_stats[categorical_test_encoded_stats[\"var\"] > .20].index\n",
    "categorical_test_encoded_sig_df = categorical_test_encoded_df[categorical_test_significant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Replacing Negative numbers in order to boxcox/log the data for normalization. \n",
    "numeric_train_df = numeric_train_df + (np.abs(numeric_train_df.min().min())) + 1\n",
    "numeric_test_df = numeric_test_df + (np.abs(numeric_test_df.min().min())) + 1\n",
    "\n",
    "#Log to normalize\n",
    "numeric_train_log_df = np.log(numeric_train_df)\n",
    "numeric_test_log_df = np.log(numeric_test_df)\n",
    "    \n",
    "#Standarizing log numeric data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numeric_train_log_df)\n",
    "\n",
    "numeric_train_log_sc = scaler.transform(numeric_train_log_df)\n",
    "numeric_test_log_sc = scaler.transform(numeric_test_log_df)\n",
    "\n",
    "numeric_train_log_sc_df = pd.DataFrame(numeric_train_log_sc, columns=numeric_train_log_df.columns)\n",
    "numeric_test_log_sc_df = pd.DataFrame(numeric_test_log_sc, columns = numeric_test_log_df.columns)\n",
    "\n",
    "#Gelman log numeric data\n",
    "numeric_train_log_gel_df = (numeric_train_log_df - numeric_train_log_df.mean()) / (2* numeric_train_log_df.std())\n",
    "numeric_test_log_gel_df = (numeric_test_log_df - numeric_test_log_df.mean()) / (2* numeric_test_log_df.std())\n",
    "\n",
    "#Removing Outlier for training dataset\n",
    "def display_outliers(data, feature, params=1.5):\n",
    "    Q1 = np.percentile(data[feature], 25)\n",
    "    Q3 = np.percentile(data[feature], 75)\n",
    "    tukey_window = params*(np.abs(Q1-Q3))\n",
    "    less_than_Q1 = data[feature] < Q1 - tukey_window\n",
    "    greater_than_Q3 = data[feature] > Q3 + tukey_window\n",
    "    tukey_mask = (less_than_Q1 | greater_than_Q3)\n",
    "    return data[tukey_mask]\n",
    "\n",
    "from collections import Counter\n",
    "def multiple_outliers(data, count = 2):\n",
    "    raw_outliers = []\n",
    "    for col in data:\n",
    "        outliers_df = display_outliers(data, col)\n",
    "        raw_outliers = raw_outliers + list(outliers_df.index)\n",
    "    outlier_count = Counter(raw_outliers)\n",
    "    outliers = [k for k, v in outlier_count.items() if v >= 6]\n",
    "    return outliers\n",
    "\n",
    "numeric_train_log_sc_out_rem_df = numeric_train_log_sc_df.drop(multiple_outliers(numeric_train_log_sc_df,6))\n",
    "numeric_train_log_gel_out_rem_df = numeric_train_log_gel_df.drop(multiple_outliers(numeric_train_log_sc_df,6))\n",
    "categorical_train_encoded_sig_out_rem_df = categorical_train_encoded_sig_df.drop(multiple_outliers(numeric_train_log_sc_df,6))\n",
    "target_out_rem = target.drop(multiple_outliers(numeric_train_log_sc_df,6))\n",
    "\n",
    "#PCA transformation. Fir the training dataset to NOT show test dataset results. \n",
    "pca_log_sc_out_rem = PCA(3)\n",
    "pca_log_gel_out_rem = PCA(3)\n",
    "\n",
    "pca_log_sc_out_rem.fit(numeric_train_log_sc_out_rem_df)\n",
    "pca_log_gel_out_rem.fit(numeric_train_log_gel_out_rem_df)\n",
    "\n",
    "\n",
    "numeric_train_log_sc_out_rem_pca_df = pd.DataFrame(pca_log_sc_out_rem.transform(numeric_train_log_sc_out_rem_df), \n",
    "                                                     columns = [\"PC 1\", \"PC 2\", \"PC 3\"], index=numeric_train_log_sc_out_rem_df.index)\n",
    "numeric_train_log_gel_out_rem_pca_df = pd.DataFrame(pca_log_gel_out_rem.transform(numeric_train_log_gel_out_rem_df), \n",
    "                                                     columns = [\"PC 1\", \"PC 2\", \"PC 3\"], index=numeric_train_log_gel_out_rem_df.index)\n",
    "numeric_test_log_sc_pca_df = pd.DataFrame(pca_log_sc_out_rem.transform(numeric_test_log_sc_df),\n",
    "                                                     columns = [\"PC 1\", \"PC 2\", \"PC 3\"], index=numeric_test_log_sc_df.index)\n",
    "numeric_test_log_gel_pca_df = pd.DataFrame(pca_log_gel_out_rem.transform(numeric_test_log_gel_df),\n",
    "                                                     columns = [\"PC 1\", \"PC 2\", \"PC 3\"], index=numeric_test_log_gel_df.index)\n",
    "\n",
    "#Merge Dataset - Note: When you merge dataset, check and make sure the index numbers are correct. \n",
    "trainset_1 = pd.merge(categorical_train_encoded_sig_out_rem_df, numeric_train_log_sc_out_rem_df, left_index=True, right_index=True)\n",
    "trainset_2 = pd.merge(trainset_1, numeric_train_log_sc_out_rem_pca_df, left_index=True, right_index=True)\n",
    "testset_1 = pd.merge(numeric_test_log_sc_df, categorical_test_encoded_sig_df, left_index=True, right_index=True)\n",
    "testset_2 = pd.merge(testset_1, numeric_test_log_sc_pca_df, left_index=True, right_index=True)\n",
    "target_1 = target_out_rem\n",
    "target_2 = target_out_rem\n",
    "\n",
    "#Del to make some room\n",
    "del train_df\n",
    "del test_df\n",
    "del Counter\n",
    "del display_outliers\n",
    "del multiple_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
